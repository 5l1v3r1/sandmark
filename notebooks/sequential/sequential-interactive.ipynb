{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout, Button, Box\n",
    "from nested_dict import nested_dict\n",
    "\n",
    "import json\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import pandas.io.json as pdjson\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_files = []\n",
    "selected_files = []\n",
    "\n",
    "artifacts_dir = \"/home/guest/artifacts\"\n",
    "artifacts_path = artifacts_dir.split(\"/artifacts/\")[0]\n",
    "\n",
    "for root, dirs, files in os.walk(artifacts_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".orun.bench\"):\n",
    "            # print(root)\n",
    "            f = root.split(\"artifacts/\")[1]\n",
    "            # print(f)\n",
    "            if (len (f.split(\"/\")) <= 5):\n",
    "                bench_files.append((os.path.join(root, file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bench_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = nested_dict(2, list)\n",
    "for x in bench_files:\n",
    "    l = x.split(\"/artifacts/\")[1]\n",
    "    d = l.split(\"/\")\n",
    "    host      = d[0]\n",
    "    repo      = d[1]\n",
    "    commit    = d[2]\n",
    "    variant   = d[3]\n",
    "    timestamp = d[4]\n",
    "    ocaml     = d[5]\n",
    "    value      = commit + \" \" + variant + \" \" + timestamp + \" \" + ocaml\n",
    "    nd[host][repo].append(value)\n",
    "benches = nd.to_dict()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x\n",
    "\n",
    "def disp(benches):\n",
    "    def select_repo(host):\n",
    "        repoW.options = benches[host]\n",
    "    \n",
    "    def select_commit(repo):\n",
    "        commitW.options = repo\n",
    "\n",
    "    def select_variant(commit):\n",
    "        return None\n",
    "\n",
    "    hostW = widgets.Dropdown(options=benches.keys(), description='Host', disabled=False)\n",
    "    hostS = hostW.value\n",
    "    hostD = widgets.interactive(select_repo, host=hostW)\n",
    "\n",
    "    repoW = widgets.Dropdown(options=benches[hostS].keys(), description='Repository', disabled=False)\n",
    "    repoS = repoW.value\n",
    "    repoD = widgets.interactive(select_commit, repo=repoW)\n",
    "\n",
    "    commitW = widgets.Dropdown(options=benches[hostS][repoS], description='Commit', disabled=False)\n",
    "    commitS = commitW.value\n",
    "    commitD = widgets.interactive(select_variant, commit=commitW)\n",
    "\n",
    "    items_layout = Layout( width='auto' )\n",
    "    \n",
    "    box_layout = Layout(display='flex',\n",
    "                       flex_flow='row wrap',\n",
    "                       align_items='flex-start',\n",
    "                       #border='solid',\n",
    "                       width='100%')\n",
    "    \n",
    "    items = [hostD, repoD, commitD]\n",
    "    box = Box(children=items, layout=box_layout)    \n",
    "    \n",
    "    # display(hostD, repoD, commitD)\n",
    "    display(box)\n",
    "    return (hostD, repoD, commitD)\n",
    "\n",
    "def get_filename(h, r, c):\n",
    "    host = h.children[0].value\n",
    "    # print(host)\n",
    "    \n",
    "    repos = list(benches[host].keys())\n",
    "    repo= repos[r.children[0].index]\n",
    "    # print(repo)\n",
    "    \n",
    "    entries = list(benches[host][repo])\n",
    "    commit_last = entries[c.children[0].index]\n",
    "\n",
    "    commit_list = commit_last.split(\" \")\n",
    "    filename = os.path.join(artifacts_path, host, repo, '/'.join(commit_list))\n",
    "    return(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = interactive(f, x=widgets.IntText(value=0, \n",
    "                                               description='Comparisons', \n",
    "                                               disabled=False))\n",
    "\n",
    "display(comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comparisons.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [[0 for x in range(4)] for y in range(comparisons.result)]\n",
    "\n",
    "for i in range(comparisons.result):\n",
    "    matrix[i][0], matrix[i][1], matrix[i][2] = disp(benches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(comparisons.result):\n",
    "    matrix[i][3] = get_filename(matrix[i][0], matrix[i][1], matrix[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (comparisons.result):\n",
    "    print(matrix[i][3])\n",
    "    selected_files.append(matrix[i][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "\n",
    "for file in selected_files:\n",
    "    with open(file) as f:\n",
    "        data = []\n",
    "        for l in f:\n",
    "            data.append(json.loads(l))\n",
    "        df = pdjson.json_normalize(data)\n",
    "        df['variant'] = os.path.basename(file).replace(\".orun.bench\",\"\")\n",
    "        data_frames.append(df)\n",
    "\n",
    "df = pd.concat (data_frames, sort=False)\n",
    "df = df.sort_values(['name']) \n",
    "# Uncomment the following to display all the lines in pandas output\n",
    "pd.set_option('display.max_rows', df.shape[0]+1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop some benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.name != 'alt-ergo.fill.why') &         #multicore version does not exist\n",
    "        (df.name != 'alt-ergo.yyll.why') &         #multicore version does not exist\n",
    "        (df.name != 'frama-c.slevel') &            #multicore version does not exist\n",
    "        (df.name != 'js_of_ocaml.frama-c_byte')]   #multicore version does not exist\n",
    "throughput_df = df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection example\n",
    "\n",
    "```\n",
    "select * from df where variant = '4.10.0+trunk' and time_secs > 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[(df['variant'] == '4.06.1+stock') & (df['time_secs'] > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['name'] == 'LU_decomposition.1024']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection example\n",
    "\n",
    "```\n",
    "select name, variant, time_secs from df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(['name','variant','time_secs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot (x='name', y='time_secs', hue='variant', data = df, kind ='bar', aspect=4)\n",
    "g.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(df,variant,topic,additionalTopics=[]):\n",
    "    df = df.sort_values([\"name\",\"variant\"])\n",
    "    grouped = df.filter(items=['name',topic,'variant']+additionalTopics).groupby('variant')\n",
    "    ndata_frames = []\n",
    "    for group in grouped:\n",
    "        (v,data) = group\n",
    "        if(v != variant):\n",
    "            data['b'+topic] = grouped.get_group(variant)[topic].values\n",
    "            data[['n'+topic]] = data[[topic]].div(grouped.get_group(variant)[topic].values, axis=0)\n",
    "            for t in additionalTopics:\n",
    "                print(variant, t)\n",
    "                data[[t]] = grouped.get_group(variant)[t].values\n",
    "            ndata_frames.append(data)\n",
    "    df = pd.concat (ndata_frames)\n",
    "    return df\n",
    "\n",
    "def plot_normalised(df,variant,topic):\n",
    "    df = pd.DataFrame.copy(df)\n",
    "    df.sort_values(by=[topic],inplace=True)\n",
    "    df[topic] = df[topic] - 1\n",
    "    g = sns.catplot (x='name', y=topic, hue='variant', data = df, kind ='bar', aspect=4, bottom=1)\n",
    "    g.set_xticklabels(rotation=90)\n",
    "    g.ax.legend(loc=8)\n",
    "    g._legend.remove()\n",
    "    g.ax.set_xlabel(\"Benchmarks\")\n",
    "    return g\n",
    "    # g.ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ndf = normalise(df, '4.06.1+stock', 'time_secs')\n",
    "plot_normalised(ndf, '4.06.1+stock','ntime_secs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top heap words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.catplot (x='name', y='gc.top_heap_words', hue='variant', data = df, kind ='bar', aspect=4)\n",
    "g.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ndf = normalise(df, '4.06.1+stock', 'gc.top_heap_words')\n",
    "plot_normalised(ndf, '4.06.1+stock','ngc.top_heap_words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max RSS (KB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.catplot (x='name', y='maxrss_kB', hue='variant', data = df, kind ='bar', aspect=4)\n",
    "g.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ndf = normalise(df,'4.06.1+stock','maxrss_kB')\n",
    "plot_normalised(ndf,'4.06.1+stock','nmaxrss_kB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.catplot (x='name', y='gc.major_collections', hue='variant', data = df, kind ='bar', aspect=4)\n",
    "g.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = normalise(df,'4.06.1+stock','gc.major_collections')\n",
    "plot_normalised(ndf,'4.06.1+stock','ngc.major_collections')\n",
    "ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot (x='name', y='gc.major_words', hue='variant', data = df, kind ='bar', aspect=4)\n",
    "g.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = normalise(df,'4.06.1+stock','gc.major_words')\n",
    "plot_normalised(ndf,'4.06.1+stock','ngc.major_words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minor Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.catplot (x='name', y='gc.minor_collections', hue='variant', data = df, kind ='bar', aspect=4)\n",
    "g.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = normalise(df,'4.06.1+stock', 'gc.minor_collections')\n",
    "plot_normalised(ndf,'4.06.1+stock', 'ngc.minor_collections')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "\n",
    "for file in glob.glob(\"*.pausetimes_*.bench\"):\n",
    "    with open(file) as f:\n",
    "        data = []\n",
    "        for l in f:\n",
    "            data.append(json.loads(l))\n",
    "        ldf = pdjson.json_normalize(data)\n",
    "        ldf['variant'] = file.replace(\".pausetimes_multicore.bench\",\"\").replace(\".pausetimes_trunk.bench\",\"\")\n",
    "        data_frames.append(ldf)\n",
    "\n",
    "df2 = pd.concat(data_frames, sort=False)\n",
    "df2 = df2.sort_values(['name'])\n",
    "\n",
    "## Drop some benchmarks\n",
    "df2 = df2[(df2.name != 'alt-ergo.fill.why') & #multicore version does not exist\n",
    "        (df2.name != 'alt-ergo.yyll.why') & #multicore version does not exist\n",
    "        (df2.name != 'frama-c.slevel') &    #multicore version does not exist\n",
    "        (df2.name != 'js_of_ocaml.frama-c_byte') &    #multicore version does not exist\n",
    "        (df2.name != 'cpdf.merge')]         #Not a macro benchmark. Will be removed from subsequent runs.\n",
    "df2.count()\n",
    "latency_df = df2\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latency distributions `distr_latency` are a list of latencies at `[10,20,30,40,50,60,70,80,90,95,99,99.9]`th percentiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.filter([\"name\",\"variant\",\"max_latency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotLatencyAt(df,at,aspect):\n",
    "    fdf = df.filter([\"name\",\"variant\",at + \"_latency\"])\n",
    "    fdf.sort_values(by=[at + '_latency'],inplace=True)\n",
    "    fdf[at + \"_latency\"] = fdf[at + \"_latency\"] / 1000.0\n",
    "    g = sns.catplot (x='name', y=at+'_latency', hue='variant', data = fdf, kind ='bar', aspect=aspect)\n",
    "    g.set_xticklabels(rotation=90)\n",
    "    g.ax.set_ylabel(at + \" latency (microseconds)\")\n",
    "    g.ax.set_xlabel(\"Benchmarks\")\n",
    "    g.ax.set_yscale('log')\n",
    "    return g\n",
    "\n",
    "plotLatencyAt(df2,\"max\",4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99.9th percentile latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLatencyAt(df,percentile,idx):\n",
    "    groups = df.groupby('variant')\n",
    "    ndfs = []\n",
    "    for group in groups:\n",
    "        (v,df) = group\n",
    "        for i, row in df.iterrows():\n",
    "            df.at[i,percentile+\"_latency\"] = list(df.at[i,\"distr_latency\"])[idx]\n",
    "        ndfs.append(df)\n",
    "    return pd.concat(ndfs)\n",
    "\n",
    "df2 = getLatencyAt(df2,\"99.9\",-1)\n",
    "plotLatencyAt(df2,\"99.9\",4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99th percentile latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = getLatencyAt(df2,\"99\",-2)\n",
    "plotLatencyAt(df2,\"99\",4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nameMap = {}\n",
    "nameMap['bdd.26'] = 'bdd'\n",
    "nameMap['binarytrees5.21'] = 'binarytrees'\n",
    "nameMap['chameneos_redux_lwt.600000'] = 'chameneos_redux_lwt'\n",
    "nameMap['cpdf.blacktext'] = 'cpdf.blacktext'\n",
    "nameMap['cpdf.scale'] = 'cpdf.scale'\n",
    "nameMap['cpdf.squeeze'] = 'cpdf.squeeze'\n",
    "nameMap['durand-kerner-aberth.'] = 'durand-kerner-aberth'\n",
    "nameMap['fannkuchredux.12'] = 'fannkuchredux'\n",
    "nameMap['fannkuchredux2.12'] = 'fannkuchredux2'\n",
    "nameMap['fasta3.25_000_000'] = 'fasta3'\n",
    "nameMap['fasta6.25_000_000'] = 'fasta6'\n",
    "nameMap['fft.'] = 'fft'\n",
    "nameMap['game_of_life.256'] = 'game_of_life'\n",
    "nameMap['kb.'] = 'kb'\n",
    "nameMap['kb_no_exc.'] = 'kb_no_exc'\n",
    "nameMap['knucleotide.'] = 'knucleotide'\n",
    "nameMap['knucleotide3.'] = 'knucleotide3'\n",
    "nameMap['levinson-durbin.'] = 'levinson-durbin'\n",
    "nameMap['lexifi-g2pp.'] = 'lexifi-g2pp'\n",
    "nameMap['lu-decomposition.'] = 'lu-decomposition'\n",
    "nameMap['mandelbrot6.16_000'] = 'mandelbrot6'\n",
    "nameMap['matrix_multiplication.1024'] = 'matrix_mult'\n",
    "nameMap['menhir.ocamly'] = 'menhir.ocamly'\n",
    "nameMap['menhir.sql-parser'] = 'menhir.sql-parser'\n",
    "nameMap['menhir.sysver'] = 'menhir.sysver'\n",
    "nameMap['minilight.roomfront'] = 'minilight.roomfront'\n",
    "nameMap['naive-multilayer.'] = 'naive-multilayer'\n",
    "nameMap['nbody.50_000_000'] = 'nbody'\n",
    "nameMap['pidigits5.10_000'] = 'pidigits5'\n",
    "nameMap['qr-decomposition.'] = 'qr-decomposition'\n",
    "nameMap['quicksort.4000000'] = 'quicksort'\n",
    "nameMap['regexredux2.'] = 'regexredux2'\n",
    "nameMap['revcomp2.'] = 'revcomp2'\n",
    "nameMap['sequence_cps.10000'] = 'sequence_cps'\n",
    "nameMap['setrip.-enc_-rseed_1067894368'] = 'setrip'\n",
    "nameMap['spectralnorm2.5_500'] = 'spectralnorm2'\n",
    "nameMap['test_decompress.64_524_288'] = 'decompress'\n",
    "nameMap['test_lwt.200'] = 'test_lwt'\n",
    "nameMap['thread_ring_lwt_mvar.20_000'] = 'thread_ring_lwt_mvar'\n",
    "nameMap['thread_ring_lwt_stream.20_000'] = 'thread_ring_lwt_stream'\n",
    "nameMap['yojson_ydump.sample.json'] = 'yojson_ydump'\n",
    "nameMap['zarith_pi.5000'] = 'zarith_pi'\n",
    "nameMap['LU_decomposition.1024'] = 'lu_decomposition'\n",
    "nameMap['floyd_warshall.512'] = 'floyd_warshall'\n",
    "\n",
    "def remapNames(n):\n",
    "    return nameMap[n]\n",
    "\n",
    "def remapVariant(v):\n",
    "    if (v.startswith('4.06.1+multicore+stw')):\n",
    "        return 'ParMinor'\n",
    "    elif (v.startswith('4.06.1+multicore')):\n",
    "        return 'ConcMinor'\n",
    "    else:\n",
    "        return 'Stock'\n",
    "    \n",
    "def sanitizeLabels(df):\n",
    "    df['name'] = df['name'].apply(remapNames)\n",
    "    df['variant'] = df['variant'].apply(remapVariant)\n",
    "\n",
    "def addBaselines(n,df,topic):\n",
    "    if (topic == \"time_secs\"):\n",
    "        baseline = round(float(df['b'+topic].loc[df['name'] == n].values[0]),2)\n",
    "        return n + \" (\" + str(baseline) + \")\"\n",
    "    elif (topic == \"gc.top_heap_words\"):\n",
    "        baseline = int(int(df['b'+topic].loc[df['name'] == n].values[0]) * 8 / (1024 * 1024))\n",
    "        return n + \" (\" + str(baseline) + \")\"\n",
    "    elif (topic == \"gc.major_collections\"):\n",
    "        bmajgcs = int(int(df['b'+topic].loc[df['name'] == n].values[0]))\n",
    "        bmajallocsmb = int(int(df['gc.major_words'].loc[df['name'] == n].values[0]) * 8 / (1024 * 1024))\n",
    "        return n + \" (\" + str(bmajgcs) + \",\" + str(bmajallocsmb) + \")\"\n",
    "    else:\n",
    "        assert(False)\n",
    "    return \n",
    "\n",
    "\n",
    "def removeDups(a):\n",
    "    m = {}\n",
    "    l = []\n",
    "    for i in a.values:\n",
    "        if not i in m.keys():\n",
    "            m[i] = 0\n",
    "            l.append(i)\n",
    "    return l\n",
    "\n",
    "def sortBasedOn(df1,groupby_topic,df2,index_topic):\n",
    "    groups = df1.groupby(groupby_topic)\n",
    "    dataframes = []\n",
    "    for g in groups:\n",
    "        (v,data) = g\n",
    "        data = data.set_index(index_topic)\n",
    "        data = data.reindex(index=removeDups(df2[index_topic]))\n",
    "        data = data.reset_index()\n",
    "        dataframes.append(data)\n",
    "    return pd.concat(dataframes)\n",
    "\n",
    "ndf_time = normalise(throughput_df, baseline,'time_secs')\n",
    "sanitizeLabels(ndf_time)\n",
    "ndf_size = normalise(throughput_df, baseline,'gc.top_heap_words')\n",
    "sanitizeLabels(ndf_size)\n",
    "ndf_majgc = normalise(throughput_df, baseline,'gc.major_collections',additionalTopics=['gc.major_words'])\n",
    "sanitizeLabels(ndf_majgc)\n",
    "\n",
    "ndf_time.sort_values(['ntime_secs'],inplace=True)\n",
    "ndf_size = sortBasedOn(ndf_size,'variant',ndf_time,'name')\n",
    "ndf_majgc = sortBasedOn(ndf_majgc,'variant',ndf_time,'name')\n",
    "\n",
    "ndf_time['name'] = ndf_time['name'].apply(addBaselines,args=(ndf_time,'time_secs'))\n",
    "ndf_size['name'] = ndf_size['name'].apply(addBaselines,args=(ndf_size,'gc.top_heap_words'))\n",
    "ndf_majgc['name'] = ndf_majgc['name'].apply(addBaselines,args=(ndf_majgc,'gc.major_collections'))\n",
    "ndf_majgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_mean(iterable):\n",
    "    a = np.array(iterable)\n",
    "    return a.prod()**(1.0/len(a))\n",
    "\n",
    "for g in ndf_time.groupby('variant'):\n",
    "    (v,df) = g\n",
    "    print(v)\n",
    "    print(geo_mean(df['ntime_secs'].values))\n",
    "    \n",
    "for g in ndf_size.groupby('variant'):\n",
    "    (v,df) = g\n",
    "    print(v)\n",
    "    print(geo_mean(df['ngc.top_heap_words'].values))\n",
    "\n",
    "for g in ndf_majgc.groupby('variant'):\n",
    "    (v,df) = g\n",
    "    print(geo_mean(df['ngc.major_collections'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.DataFrame.copy(ndf_time)\n",
    "_df['ntime_secs'] = _df['ntime_secs'] - 1 # cf [bottom=1]\n",
    "g = sns.catplot (x='name', y='ntime_secs', hue='variant', data = _df, \n",
    "                 kind ='bar', aspect=4, height=3, bottom=1)\n",
    "g.set_xticklabels(rotation=90)\n",
    "g.ax.set_ylim(0.75,1.35)\n",
    "g.ax.legend(loc=2)\n",
    "g._legend.remove()\n",
    "g.ax.set_xlabel(\"Benchmarks\")\n",
    "g.ax.set_ylabel(\"Normalized Time\")\n",
    "g.savefig('seq_time.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_df = pd.DataFrame.copy(ndf_size)\n",
    "_df['ngc.top_heap_words'] = _df['ngc.top_heap_words'] - 1 # cf [bottom=1]\n",
    "g = sns.catplot (x='name', y='ngc.top_heap_words', hue='variant', \n",
    "                 data = _df, kind ='bar', aspect=4, bottom=1,height=3)\n",
    "g.set_xticklabels(rotation=90)\n",
    "g.ax.legend(loc=3)\n",
    "g._legend.remove()\n",
    "g.ax.set_xlabel(\"Benchmarks\")\n",
    "g.ax.set_ylabel(\"Normalized Max Heap Size\")\n",
    "g.savefig('seq_max_heap_size.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.DataFrame.copy(ndf_majgc)\n",
    "_df['ngc.major_collections'] = _df['ngc.major_collections'] - 1 # cf [bottom=1]\n",
    "g = sns.catplot (x='name', y='ngc.major_collections', hue='variant', \n",
    "                 data = _df, kind ='bar', aspect=4, bottom=1,height=3)\n",
    "g.set_xticklabels(rotation=90)\n",
    "g.ax.legend(loc=1)\n",
    "g._legend.remove()\n",
    "g.ax.set_xlabel(\"Benchmarks\")\n",
    "g.ax.set_ylabel(\"Normalized Major GC Count\")\n",
    "g.savefig('seq_majgc_count.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = pd.DataFrame.copy(latency_df)\n",
    "ldf['name'] = ldf['name'].apply(remapNames)\n",
    "ldf['variant'] = ldf['variant'].apply(remapVariant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLatencyAt2(df,at):\n",
    "    fdf = df.filter([\"name\",\"variant\",at + \"_latency\"])\n",
    "    fdf.sort_values(by=[at + '_latency'],inplace=True)\n",
    "    fdf[at + \"_latency\"] = fdf[at + \"_latency\"] / 1000.0\n",
    "    g = sns.catplot (x='name', y=at+'_latency', hue='variant', data = fdf, kind ='bar', height=3, aspect=4)\n",
    "    g.set_xticklabels(rotation=90)\n",
    "    g.ax.set_ylabel(at + \" latency (microseconds)\")\n",
    "    g.ax.set_xlabel(\"Benchmarks\")\n",
    "    g.ax.set_yscale('log')\n",
    "    return g\n",
    "\n",
    "g = plotLatencyAt2(ldf,\"max\")\n",
    "g.ax.legend(loc=2)\n",
    "g._legend.remove()\n",
    "g.savefig('seq_max_latency.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = getLatencyAt(ldf,\"99.9\",-1)\n",
    "g = plotLatencyAt2(ldf,\"99.9\")\n",
    "g.ax.legend(loc=2)\n",
    "g._legend.remove()\n",
    "g.savefig('seq_999_latency.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf.loc[ldf.name == 'menhir.ocamly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
